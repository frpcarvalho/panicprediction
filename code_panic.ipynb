{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_base_name(filename):\n",
    "    \"\"\"Extrai o nome base do arquivo, removendo sufixos _A, _B, _C e .xpt\"\"\"\n",
    "    name = filename.replace('.xpt', '')\n",
    "    if name.endswith('_A') or name.endswith('_B') or name.endswith('_C'):\n",
    "        return name[:-2]\n",
    "    return name\n",
    "\n",
    "def find_common_datasets(base_paths):\n",
    "    \"\"\"Encontra todos os datasets que existem nos três ciclos\"\"\"\n",
    "    datasets_by_cycle = {}\n",
    "    \n",
    "    # Coletar nomes base de cada ciclo\n",
    "    for path in base_paths:\n",
    "        cycle = os.path.basename(path)\n",
    "        files = [f for f in os.listdir(path) if f.lower().endswith('.xpt')]\n",
    "        datasets_by_cycle[cycle] = set(get_base_name(f) for f in files)\n",
    "    \n",
    "    # Encontrar interseção dos conjuntos\n",
    "    common_datasets = set.intersection(*datasets_by_cycle.values())\n",
    "    return sorted(list(common_datasets))\n",
    "\n",
    "def merge_dataset(base_name, base_paths):\n",
    "    \"\"\"Combina um dataset específico dos três ciclos\"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    for path in base_paths:\n",
    "        cycle = os.path.basename(path)\n",
    "        \n",
    "        # Tentar encontrar o arquivo com o padrão correto\n",
    "        if cycle == 'NHANES_1999_2000':\n",
    "            possible_names = [f\"{base_name}.xpt\", f\"{base_name}_A.xpt\"]\n",
    "        elif cycle == 'NHANES_2001_2002':\n",
    "            possible_names = [f\"{base_name}_B.xpt\"]\n",
    "        else:  # 2003-2004\n",
    "            possible_names = [f\"{base_name}_C.xpt\"]\n",
    "        \n",
    "        file_found = False\n",
    "        for file_name in possible_names:\n",
    "            full_path = os.path.join(path, file_name)\n",
    "            if os.path.exists(full_path):\n",
    "                try:\n",
    "                    print(f\"Lendo {file_name}\")\n",
    "                    df = pd.read_sas(full_path)\n",
    "                    \n",
    "                    # Padronizar nomes das colunas\n",
    "                    df.columns = [col[:-2] if col.endswith(('_A', '_B', '_C')) else col \n",
    "                                for col in df.columns]\n",
    "                    \n",
    "                    df['NHANES_CYCLE'] = cycle\n",
    "                    dfs.append(df)\n",
    "                    print(f\"Linhas: {df.shape[0]}, Colunas: {df.shape[1]}\")\n",
    "                    file_found = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao ler {file_name}: {str(e)}\")\n",
    "        \n",
    "        if not file_found:\n",
    "            print(f\"Nenhum arquivo encontrado para {base_name} em {cycle}\")\n",
    "            return None\n",
    "    \n",
    "    if len(dfs) == 3:\n",
    "        # Encontrar colunas comuns\n",
    "        common_cols = set.intersection(*[set(df.columns) for df in dfs]) - {'NHANES_CYCLE'}\n",
    "        columns_to_keep = list(common_cols) + ['NHANES_CYCLE']\n",
    "        \n",
    "        # Combinar os datasets\n",
    "        combined_df = pd.concat([df[columns_to_keep] for df in dfs], \n",
    "                              axis=0, ignore_index=True)\n",
    "        print(f\"Dataset combinado - Linhas: {combined_df.shape[0]}, Colunas: {combined_df.shape[1]}\")\n",
    "        return combined_df\n",
    "    return None\n",
    "\n",
    "# Caminhos para os diretórios\n",
    "base_paths = [\n",
    "    '/Users/filipecarvalho/Documents/data_science_projects/PANICPRED/NHANES_1999_2000',\n",
    "    '/Users/filipecarvalho/Documents/data_science_projects/PANICPRED/NHANES_2001_2002',\n",
    "    '/Users/filipecarvalho/Documents/data_science_projects/PANICPRED/NHANES_2003_2004'\n",
    "]\n",
    "\n",
    "# Criar pasta para outputs\n",
    "output_dir = os.path.join(os.path.dirname(base_paths[0]), 'combined_datasets')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Arquivos combinados serão salvos em: {output_dir}\")\n",
    "\n",
    "# Encontrar todos os datasets comuns\n",
    "print(\"\\nProcurando datasets comuns aos três ciclos...\")\n",
    "common_datasets = find_common_datasets(base_paths)\n",
    "print(f\"Encontrados {len(common_datasets)} datasets comuns:\")\n",
    "for dataset in common_datasets:\n",
    "    print(f\"- {dataset}\")\n",
    "\n",
    "# Processar cada dataset\n",
    "print(\"\\nIniciando processo de merge...\")\n",
    "successful_merges = []\n",
    "failed_merges = []\n",
    "\n",
    "for dataset in common_datasets:\n",
    "    print(f\"\\nProcessando {dataset}...\")\n",
    "    combined_df = merge_dataset(dataset, base_paths)\n",
    "    \n",
    "    if combined_df is not None:\n",
    "        output_path = os.path.join(output_dir, f'{dataset}_combined.csv')\n",
    "        combined_df.to_csv(output_path, index=False)\n",
    "        successful_merges.append(dataset)\n",
    "        print(f\"Dataset {dataset} salvo com sucesso\")\n",
    "    else:\n",
    "        failed_merges.append(dataset)\n",
    "        print(f\"Falha ao combinar dataset {dataset}\")\n",
    "\n",
    "print(\"\\nProcessamento concluído!\")\n",
    "print(f\"\\nDatasets combinados com sucesso ({len(successful_merges)}):\")\n",
    "for dataset in successful_merges:\n",
    "    print(f\"- {dataset}\")\n",
    "\n",
    "if failed_merges:\n",
    "    print(f\"\\nDatasets que falharam ({len(failed_merges)}):\")\n",
    "    for dataset in failed_merges:\n",
    "        print(f\"- {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def merge_panic_data(base_paths):\n",
    "    \"\"\"Combina os dados de pânico dos três ciclos\"\"\"\n",
    "    dfs = []\n",
    "    \n",
    "    # Definir os nomes específicos para cada ciclo\n",
    "    file_names = {\n",
    "        'NHANES_1999_2000': 'CIQPANIC.xpt',\n",
    "        'NHANES_2001_2002': 'CIQPAN_B.xpt',\n",
    "        'NHANES_2003_2004': 'CIQPAN_C.xpt'\n",
    "    }\n",
    "    \n",
    "    for path in base_paths:\n",
    "        cycle = os.path.basename(path)\n",
    "        file_name = file_names[cycle]\n",
    "        full_path = os.path.join(path, file_name)\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(full_path):\n",
    "                print(f\"Lendo {file_name} do ciclo {cycle}\")\n",
    "                df = pd.read_sas(full_path)\n",
    "                \n",
    "                # Padronizar nomes das colunas (remover sufixos _B e _C)\n",
    "                df.columns = [col[:-2] if col.endswith(('_B', '_C')) else col \n",
    "                            for col in df.columns]\n",
    "                \n",
    "                # Adicionar coluna identificando o ciclo\n",
    "                df['NHANES_CYCLE'] = cycle\n",
    "                \n",
    "                dfs.append(df)\n",
    "                print(f\"Linhas: {df.shape[0]}, Colunas: {df.shape[1]}\")\n",
    "            else:\n",
    "                print(f\"Arquivo não encontrado: {full_path}\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {file_name}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    if len(dfs) == 3:\n",
    "        # Encontrar colunas comuns\n",
    "        common_cols = set.intersection(*[set(df.columns) for df in dfs]) - {'NHANES_CYCLE'}\n",
    "        columns_to_keep = list(common_cols) + ['NHANES_CYCLE']\n",
    "        \n",
    "        # Combinar os datasets\n",
    "        combined_df = pd.concat([df[columns_to_keep] for df in dfs], \n",
    "                              axis=0, ignore_index=True)\n",
    "        print(f\"\\nDataset combinado:\")\n",
    "        print(f\"Linhas totais: {combined_df.shape[0]}\")\n",
    "        print(f\"Colunas: {combined_df.shape[1]}\")\n",
    "        return combined_df\n",
    "    return None\n",
    "\n",
    "# Caminhos para os diretórios\n",
    "base_paths = [\n",
    "    '/Users/filipecarvalho/Documents/data_science_projects/SadPred/NHANES_1999_2000',\n",
    "    '/Users/filipecarvalho/Documents/data_science_projects/SadPred/NHANES_2001_2002',\n",
    "    '/Users/filipecarvalho/Documents/data_science_projects/SadPred/NHANES_2003_2004'\n",
    "]\n",
    "\n",
    "# Criar pasta para outputs se ainda não existir\n",
    "output_dir = os.path.join(os.path.dirname(base_paths[0]), 'combined_datasets')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Iniciando combinação dos dados de pânico...\")\n",
    "combined_df = merge_panic_data(base_paths)\n",
    "\n",
    "if combined_df is not None:\n",
    "    output_path = os.path.join(output_dir, 'PANIC_combined.csv')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nDataset combinado salvo em: {output_path}\")\n",
    "    print(\"\\nPrimeiras colunas do dataset combinado:\")\n",
    "    print(list(combined_df.columns)[:5])\n",
    "else:\n",
    "    print(\"\\nNão foi possível combinar os datasets de pânico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Caminho para o arquivo combinado\n",
    "file_path = '/Users/filipecarvalho/Documents/data_science_projects/PANICPRED/combined_datasets/PANIC_combined.csv'\n",
    "\n",
    "# Lista de variáveis para verificar\n",
    "variables_to_check = [\n",
    "    'CIQP44A',  # Strong Fear: Giving a speech\n",
    "    'CIQP44B',  # Strong Fear: Party or social event\n",
    "    'CIQP44C',  # Strong Fear: Being in a crowd\n",
    "    'CIQP44D',  # Strong Fear: Meeting new people\n",
    "    'CIQP44E',  # Strong Fear: Being outside, away\n",
    "    'CIQP44F',  # Strong Fear: Traveling bus, train, car\n",
    "    'CIQP44G',  # Strong Fear: Crowd, standing in line\n",
    "    'CIQP44H'   # Strong Fear: Being in a public place\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Ler o dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Total de colunas no dataset: {len(df.columns)}\")\n",
    "    print(\"\\nVerificando variáveis específicas:\")\n",
    "    \n",
    "    # Verificar cada variável\n",
    "    for var in variables_to_check:\n",
    "        if var in df.columns:\n",
    "            print(f\"✓ {var} - Presente\")\n",
    "            # Mostrar alguns valores únicos para confirmar o conteúdo\n",
    "            unique_values = sorted(df[var].unique())\n",
    "            print(f\"   Valores únicos: {unique_values}\")\n",
    "        else:\n",
    "            print(f\"✗ {var} - Ausente\")\n",
    "    \n",
    "    print(\"\\nTodas as colunas disponíveis:\")\n",
    "    for col in sorted(df.columns):\n",
    "        print(f\"- {col}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler o arquivo: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Pasta dos datasets combinados\n",
    "combined_dir = '/Users/filipecarvalho/Documents/data_science_projects/PANICPRED/combined_datasets'\n",
    "\n",
    "# Verificar cada arquivo CSV\n",
    "print(\"Verificando coluna SEQN em todos os datasets combinados:\\n\")\n",
    "\n",
    "try:\n",
    "    for file in sorted(os.listdir(combined_dir)):\n",
    "        if file.endswith('_combined.csv'):\n",
    "            file_path = os.path.join(combined_dir, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Verificar se SEQN está presente\n",
    "            has_seqn = 'SEQN' in df.columns\n",
    "            \n",
    "            print(f\"{file:30} - SEQN: {'✓ Presente' if has_seqn else '✗ Ausente'}\")\n",
    "            \n",
    "            if has_seqn:\n",
    "                # Verificar se existem valores nulos\n",
    "                null_count = df['SEQN'].isnull().sum()\n",
    "                if null_count > 0:\n",
    "                    print(f\"   Atenção: {null_count} valores nulos encontrados!\")\n",
    "                \n",
    "                # Verificar se os valores são únicos\n",
    "                unique_count = df['SEQN'].nunique()\n",
    "                total_count = len(df)\n",
    "                if unique_count != total_count:\n",
    "                    print(f\"   Atenção: {unique_count} valores únicos em {total_count} linhas\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao processar arquivos: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def merge_all_datasets():\n",
    "    # Pasta dos datasets combinados\n",
    "    combined_dir = '/Users/filipecarvalho/Documents/data_science_projects/PANICPRED/combined_datasets'\n",
    "    \n",
    "    # Lista para armazenar todos os dataframes\n",
    "    all_dfs = []\n",
    "    \n",
    "    print(\"Iniciando processo de merge...\\n\")\n",
    "    \n",
    "    # Ler cada arquivo CSV\n",
    "    for file in sorted(os.listdir(combined_dir)):\n",
    "        if file.endswith('_combined.csv'):\n",
    "            print(f\"Lendo {file}\")\n",
    "            df = pd.read_csv(os.path.join(combined_dir, file))\n",
    "            \n",
    "            # Extrair nome do dataset do nome do arquivo\n",
    "            dataset_name = file.replace('_combined.csv', '')\n",
    "            \n",
    "            # Adicionar prefixo às colunas exceto SEQN e NHANES_CYCLE\n",
    "            columns_to_rename = [col for col in df.columns if col not in ['SEQN', 'NHANES_CYCLE']]\n",
    "            df = df.rename(columns={col: f\"{dataset_name}_{col}\" for col in columns_to_rename})\n",
    "            \n",
    "            print(f\"- Linhas: {len(df)}\")\n",
    "            print(f\"- Colunas: {len(df.columns)}\")\n",
    "            all_dfs.append(df)\n",
    "    \n",
    "    # Realizar o merge de todos os dataframes\n",
    "    print(\"\\nRealizando merge dos datasets...\")\n",
    "    final_df = all_dfs[0]\n",
    "    \n",
    "    for i, df in enumerate(all_dfs[1:], 1):\n",
    "        print(f\"\\nMerge {i}\")\n",
    "        print(f\"- Linhas antes do merge: {len(final_df)}\")\n",
    "        final_df = pd.merge(final_df, df, on=['SEQN', 'NHANES_CYCLE'], how='outer')\n",
    "        print(f\"- Linhas após o merge: {len(final_df)}\")\n",
    "    \n",
    "    # Estatísticas finais\n",
    "    print(\"\\nEstatísticas do dataset final:\")\n",
    "    print(f\"Total de linhas: {len(final_df)}\")\n",
    "    print(f\"Total de colunas: {len(final_df.columns)}\")\n",
    "    \n",
    "    # Verificar valores nulos\n",
    "    null_counts = final_df.isnull().sum()\n",
    "    print(\"\\nColunas com mais valores nulos:\")\n",
    "    print(null_counts[null_counts > 0].sort_values(ascending=False).head())\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Executar o merge\n",
    "print(\"Iniciando processo de merge completo...\")\n",
    "final_dataset = merge_all_datasets()\n",
    "\n",
    "# Salvar o resultado\n",
    "output_path = '/Users/filipecarvalho/Documents/data_science_projects/SadPred/NHANES_complete.csv'\n",
    "final_dataset.to_csv(output_path, index=False)\n",
    "print(f\"\\nDataset completo salvo em: {output_path}\")\n",
    "\n",
    "# Mostrar primeiras linhas\n",
    "print(\"\\nPrimeiras linhas do dataset final:\")\n",
    "print(final_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler o dataset de pânico para obter os SEQNs\n",
    "panic_df = pd.read_csv('/Users/filipecarvalho/Documents/data_science_projects/PANICPRED/combined_datasets/PANIC_combined.csv')\n",
    "\n",
    "# Obter lista única de SEQNs do dataset de pânico\n",
    "panic_seqns = set(panic_df['SEQN'])\n",
    "print(f\"Número de SEQNs únicos no dataset de pânico: {len(panic_seqns)}\")\n",
    "\n",
    "# Ler o dataset completo\n",
    "complete_df = pd.read_csv('/Users/filipecarvalho/Documents/data_science_projects/SadPred/NHANES_complete.csv')\n",
    "print(f\"Número de linhas no dataset completo: {len(complete_df)}\")\n",
    "\n",
    "# Criar subset usando apenas os SEQNs do dataset de pânico\n",
    "subset_df = complete_df[complete_df['SEQN'].isin(panic_seqns)]\n",
    "print(f\"Número de linhas no subset final: {len(subset_df)}\")\n",
    "\n",
    "# Verificar se mantivemos todos os SEQNs do panic\n",
    "final_seqns = set(subset_df['SEQN'])\n",
    "missing_seqns = panic_seqns - final_seqns\n",
    "if missing_seqns:\n",
    "    print(f\"\\nAtenção: {len(missing_seqns)} SEQNs do dataset de pânico não foram encontrados no dataset completo\")\n",
    "else:\n",
    "    print(\"\\nTodos os SEQNs do dataset de pânico foram mantidos no subset\")\n",
    "\n",
    "# Salvar o subset\n",
    "output_path = '/Users/filipecarvalho/Documents/data_science_projects/SadPred/NHANES_panic_subset.csv'\n",
    "subset_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nSubset salvo em: {output_path}\")\n",
    "\n",
    "# Mostrar algumas estatísticas do subset\n",
    "print(\"\\nEstatísticas do subset:\")\n",
    "print(f\"Número de colunas: {len(subset_df.columns)}\")\n",
    "print(\"\\nDistribuição por ciclo:\")\n",
    "print(subset_df['NHANES_CYCLE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ler o dataset\n",
    "df = pd.read_csv(\"/Users/filipecarvalho/Documents/data_science_projects/PanicPred/NHANES_panic_subset.csv\")\n",
    "\n",
    "# Criar a variável target baseada em CIQGAD_CIDGSCOR = 1\n",
    "df['target'] = (df['PANIC_CIDPSCOR'] == 1).astype(int)\n",
    "\n",
    "# Calcular estatísticas gerais\n",
    "total = len(df)\n",
    "positivos = df['target'].sum()\n",
    "negativos = total - positivos\n",
    "\n",
    "print(\"Estatísticas da variável target (PANIC_CIDPSCOR = 1):\")\n",
    "print(f\"Total de observações: {total}\")\n",
    "print(f\"Casos positivos: {positivos} ({(positivos/total)*100:.2f}%)\")\n",
    "print(f\"Casos negativos: {negativos} ({(negativos/total)*100:.2f}%)\")\n",
    "\n",
    "# Análise por ciclo\n",
    "print(\"\\nDistribuição por ciclo NHANES:\")\n",
    "for cycle in df['NHANES_CYCLE'].unique():\n",
    "    cycle_df = df[df['NHANES_CYCLE'] == cycle]\n",
    "    cycle_pos = cycle_df['target'].sum()\n",
    "    cycle_total = len(cycle_df)\n",
    "    print(f\"\\n{cycle}:\")\n",
    "    print(f\"Total: {cycle_total}\")\n",
    "    print(f\"Positivos: {cycle_pos} ({(cycle_pos/cycle_total)*100:.2f}%)\")\n",
    "\n",
    "# Salvar o dataset com a nova variável target\n",
    "df.to_csv('/Users/filipecarvalho/Documents/data_science_projects/PANICPRED/NHANES_panic_subset_with_target.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import random\n",
    "import shap\n",
    "# Ler dados\n",
    "df = pd.read_csv('NHANES_panic_subset_with_target.csv')\n",
    "\n",
    "# Lista de variáveis para transformar em categóricas\n",
    "categorical_vars = ['MCQ_MCQ250C', 'MCQ_MCQ250F', 'DEMO_INDHHINC', 'MCQ_MCQ250A', \n",
    "                   'SMQ_SMQ040', 'BPQ_BPQ020', 'BPQ_BPQ060', 'DEMO_DMDEDUC', \n",
    "                   'BPQ_BPQ040D', 'MCQ_MCQ160A', 'ALQ_ALQ150', 'PAQ_PAD320']\n",
    "\n",
    "\n",
    "# Para verificar os tipos atuais\n",
    "for var in categorical_vars:\n",
    "    print(f\"\\nTipo atual de {var}:\", df[var].dtype)\n",
    "\n",
    "# Função auxiliar para fazer comparação segura\n",
    "def safe_comparison(series, value):\n",
    "    # Converte para numérico (mantém tanto int como float)\n",
    "    numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "    return numeric_series == value\n",
    "\n",
    "def safe_isin(series, values):\n",
    "    # Converte para numérico (mantém tanto int como float)\n",
    "    numeric_series = pd.to_numeric(series, errors='coerce')\n",
    "    return numeric_series.isin(values)\n",
    "\n",
    "# Fazer as substituições\n",
    "df.loc[safe_comparison(df['MCQ_MCQ250C'], 9), 'MCQ_MCQ250C'] = 2\n",
    "\n",
    "df.loc[safe_isin(df['MCQ_MCQ250F'], [7, 9]), 'MCQ_MCQ250F'] = 2\n",
    "df.loc[safe_isin(df['MCQ_MCQ250A'], [7, 9]), 'MCQ_MCQ250A'] = 2\n",
    "\n",
    "# Para DEMO_INDHHINC\n",
    "df.loc[safe_comparison(df['DEMO_INDHHINC'], 13), 'DEMO_INDHHINC'] = [random.randint(1,4) for _ in range(len(df[safe_comparison(df['DEMO_INDHHINC'], 13)]))]\n",
    "df.loc[safe_comparison(df['DEMO_INDHHINC'], 12), 'DEMO_INDHHINC'] = [random.randint(5,11) for _ in range(len(df[safe_comparison(df['DEMO_INDHHINC'], 12)]))]\n",
    "df.loc[safe_isin(df['DEMO_INDHHINC'], [77, 99]), 'DEMO_INDHHINC'] = np.nan\n",
    "\n",
    "# Para as outras variáveis\n",
    "df.loc[safe_isin(df['BPQ_BPQ020'], [7, 9]), 'BPQ_BPQ020'] = 2\n",
    "df.loc[safe_isin(df['BPQ_BPQ060'], [7, 9]), 'BPQ_BPQ060'] = 2\n",
    "\n",
    "# DEMO_DMDEDUC\n",
    "df.loc[safe_isin(df['DEMO_DMDEDUC'], [7, 9]), 'DEMO_DMDEDUC'] = [random.randint(1,3) for _ in range(len(df[safe_isin(df['DEMO_DMDEDUC'], [7, 9])]))]\n",
    "\n",
    "df.loc[safe_isin(df['BPQ_BPQ040D'], [7, 9]), 'BPQ_BPQ040D'] = 2\n",
    "df.loc[safe_isin(df['MCQ_MCQ160A'], [7, 9]), 'MCQ_MCQ160A'] = 2\n",
    "df.loc[safe_isin(df['ALQ_ALQ150'], [7, 9]), 'ALQ_ALQ150'] = 2\n",
    "df.loc[safe_comparison(df['PAQ_PAD320'], 3), 'PAQ_PAD320'] = np.nan\n",
    "\n",
    "# Para verificar as mudanças\n",
    "for var in categorical_vars:\n",
    "    print(f\"\\nDistribuição de {var}:\")\n",
    "    print(df[var].value_counts(dropna=False))\n",
    "\n",
    "# Analisar a distribuição por gênero e target\n",
    "print(\"Distribuição inicial por gênero e target:\")\n",
    "print(pd.crosstab(df['DEMO_RIAGENDR'], df['target']))\n",
    "\n",
    "# Separar colunas numéricas e categóricas\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Pré-processar dados\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Imputar valores numéricos coluna por coluna\n",
    "for col in numeric_cols:\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_processed[col] = imputer.fit_transform(df_processed[[col]])\n",
    "\n",
    "# Processar dados categóricos\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    # Primeiro imputar valores faltantes\n",
    "    df_processed[col] = df_processed[col].fillna('missing')\n",
    "    # Depois aplicar LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "# Variáveis a remover (mesma lista do código original)\n",
    "bix_vars_to_remove = ['BIX_BIXC245K', 'BIX_BIXC339K', 'BIX_BIXC083K',\n",
    "                      \"BIX_BIXC200K\", \"BIX_BIXC067K\",\"BIX_BIXC378K\",\n",
    "                      \"BIX_BIXC159K\", \"BIX_BIXC012K\", \"BIX_BIXC054K\",\n",
    "                      \"BIX_BIXC421K\", \"BIX_BIXC060K\", \"BIX_BIXC100K\",\n",
    "                      \"BIX_BIXC128K\", \"BIX_BIXC075K\", \"BIX_BIXC469K\",\n",
    "                      \"BIX_BIXC093K\", \"BIX_BIXC177K\", \"BIX_BIXC115K\",\n",
    "                      \"BIX_BIXC013K\", \"BIX_BIXC016K\", \"BIX_BIXC143K\",\n",
    "                      \"BIX_BIXC035K\", \"BIX_BIXC014K\", \"BIX_BIXC028K\",\n",
    "                      \"BIX_BIXC015K\", \"BIX_BIXC043K\", \"BIX_BIXC018K\",\n",
    "                      \"BIX_BIXC011K\", \"BIX_BIXC220K\", \"BIX_BIXC273K\",\n",
    "                      \"BIX_BIXC500K\", \"BIX_BIXC050K\",\"CIQGAD_CIQG02\",\n",
    "                      \"BIX_BIXC020K\", \"BIX_BIXC806K\", \"BIX_BIXS005K\", \n",
    "                      \"BIX_BIXC031K\", \"BIX_BIXC025K\", \"BIX_BIXC009K\",\n",
    "                      \"BIX_BIXC023K\", \"BIX_BIXC649K\", \"BIX_BIXC304K\", \n",
    "                      \"BIX_BIXC723K\",\"BIX_BIXC582K\",\"BIX_BIXS013K\", \"BIX_BIDICF\",\n",
    "                      \"BIX_BIXC898K\", \"MPQ_MPD040\",\"MPQ_MPQ020\",\"MCQ_MCQ180K\",\n",
    "                      \"CIQGAD_CIDGSCOR\" ,\"MPQ_MPQ100\",\"CIQGAD_CIQG01\",\n",
    "                      \"CIQGAD_CIQG05\",\"CIQGAD_CIQG33\", \"CIQGAD_CIQG15\", \n",
    "                      \"CIQGAD_CIQG03\", \"CIQGAD_CIQG23\", \"CIQGAD_CIQG30\",\n",
    "                      \"CIQGAD_CIQG29\", \"CIQGAD_CIQG26\", \"CIQGAD_CIQG16\", \n",
    "                      \"CIQGAD_CIQG06\", \"CIQGAD_CIQG04\", \"CIQGAD_CIQG20\", \n",
    "                       \"RXQ_RX_RXDDRGID\", \"HUQ_HUQ090\", \"CIQGAD_CIQG28\",\n",
    "                      \"CIQGAD_CIQG38\", \"CIQGAD_CIQG24\", \"CIQGAD_CIQG35\",\n",
    "                      \"CIQGAD_CIQG17G\", \"CIQGAD_CIQG12\", \"CIQGAD_CIQG36\",\n",
    "                      \"CIQGAD_CIQG09\", \"BIX_BIXC039K\", \"BIX_BIXS016K\",\n",
    "                      \"HUQ_HUQ050\", \"CIQGAD_CIQG25\", \"HUQ_HUQ010\", \"HSQ_HSQ520\",\n",
    "                       \"BPQ_BPQ040B\", \"MPQ_MPQ110\", \"MCQ_MCQ245A\", \"HIQ_HIQ220\",\n",
    "                         \"AUQ_AUQ190\", \"SSMUMP_SSISR\", \"AUQ_AUQ200\", \n",
    "                         \"CIQGAD_CIQG08\", \"PFQ_PFQ059\", \"BIX_BIDTD\", \"DEMO_DMDHRAGE\",\n",
    "                          \"DEMO_DMDHREDU\", \"ALQ_ALQ110\", \"BIX_BIDFIT\", \"BPQ_BPQ040C\",\n",
    "                            \"DEMO_DMDHRBRN\", \"HUQ_HUQ030\", \"RDQ_RDQ070\", \"RHQ_RHD080\",\n",
    "                              \"RHQ_RHQ210\", \"RHQ_RHQ250\", \"BIX_BIXC1M\", \"CVX_CVDR3HR\",\n",
    "                                \"OCQ_OCQ380\", \"SMQ_SMD100TR\", \"RHQ_RHQ190\", \"RHQ_RHQ160\",\n",
    "                    \"CVX_CVDEXCL2\", \"HSQ_HSQ500\", \"OCQ_OCQ260\", \"PAQ_PAQ500\", \"DEMO_SDDSRVYR\",\n",
    "                    \"RHQ_RHQ460Q\", \"UC_URXPREG\", \"CVX_CVDR3SY\", \"SMQ_SMD100BR\",\n",
    "                       \"BIX_BIDFC\", \"BPX_BPXSY3\", \"SMQ_SMD100CO\", \"MCQ_MCQ245B\", \"BPX_BPXSAR\",\n",
    "                        \"SMQ_SMD100LN\", \"VIX_VIXKLM2\", \"SMQ_SMD100NI\", \"VIX_VIXPLS\", \"SMQ_SMD130\", \n",
    "                          \"BIX_BIDECF\", \"BIX_BIAEXSTS\", \"BPX_BPXSY2\", \"RHQ_RHQ460U\", \"RHQ_RHQ460U\",\n",
    "                            \"SMQ_SMD100MN\", \"BIX_BIDTBW\", \"FSQ_FSD170N\", \"RHQ_RHQ430\",\n",
    "                              \"HUQ_HUQ020\", \"OCQ_OCD395\", \"BMX_BMXWAIST\", \"HSQ_HSQ510\",\n",
    "                                \"MPQ_MPQ060\", \"HOQ_HOD050\", \"VIX_VIXPRC\",\n",
    "                                  \"OHQ_OHQ033\",\n",
    "    \"CVX_CVDEXLEN\",\n",
    "    \"BIX_BIDALPHA\",\n",
    "    \"SMQFAM_SMD430\", \"BIX_BIXS031K\",\n",
    "    \"DIQ_DIQ060U\", \"VIX_VIXOLSM\", \"DEMO_RIDEXPRG\", \"RHQ_RHQ564\", \"RDQ_RDQ137\",\n",
    "     \"DEMO_DMDHRGND\", \"OHXREF_OHAEXSTS\", \"CIQGAD_CIQG10\", \"OCQ_OCD230\", \"OHQ_OHQ030\",\n",
    "      \"BPX_BPXDI4\", \"HOQ_HOD030\", \"PH_PHACOFHR\", \"RHQ_RHQ568Q\", \"DIQ_DIQ050\",\n",
    "       \"DEMO_RIDEXMON\", \"WHQ_WHQ030\", \"AUQ_AUQ210\", \"CVX_CVAPROT\", \"PH_PHACOFMN\", \"BPQ_BPQ040F\",\n",
    "        \"OHXREF_OHQ134\", \"CVX_CVDPROT\", \"VIX_VIXORAM\", \"OHXREF_OHQ132\", \"OCQ_OCD480\",\n",
    "         \"BIX_BIXC005K\", \"BPX_BPXSY4\", \"SXQ_SXQ100\", \"CIQGAD_CIQG32\", \"RDQ_RDQ135\", \"PAQ_PAD080\",\n",
    "          \"VIX_VIDROVA\", \"CVX_CVDS2SY\", \"MCQ_MCQ180A\", \"OSQ_OSD030BA\", \"RHQ_RHQ390\", \"CIQGAD_CIQG32\",\n",
    "           \"SMQ_SMD190\", \"RHQ_RHQ420\", \"RHQ_RHQ330\", \"PH_PHAFSTHR\", \"RHQ_RHQ180\", \"MPQ_MPQ090\",\n",
    "            \"DEMO_WTMEC2YR\", \"DEMO_RIDAGEEX\", \"BIX_BIXS025K\", \"VIX_VIXKLM1\", \"VIX_VIXKRM1\",\n",
    "             \"CIQGAD_WTSCI2YR\", \"BIX_BIDPFAT\", \"VIX_VIXOLCM\", \"BIX_BIXS035K\",\n",
    "              \"BIX_BIXS039K\", \"VIX_VIXORCM\" ]\n",
    "# Separar features e target \n",
    "columns_to_drop = [col for col in df.columns if 'PANIC_' in col] + ['SEQN'] + bix_vars_to_remove\n",
    "X = df_processed.drop(columns_to_drop + ['target'], axis=1)\n",
    "y = df_processed['target']\n",
    "\n",
    "# Primeiro dividir dados em treino e teste para evitar data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Criar DataFrame de treino completo\n",
    "train_data = X_train.copy()\n",
    "train_data['target'] = y_train\n",
    "\n",
    "# Separar classe minoritária\n",
    "train_minority = train_data[train_data['target'] == 1]\n",
    "train_majority = train_data[train_data['target'] == 0]\n",
    "\n",
    "print(\"\\nDistribuição de gênero na classe minoritária antes do balanceamento:\")\n",
    "print(pd.crosstab(train_minority['DEMO_RIAGENDR'], columns='count'))\n",
    "\n",
    "# SMOTE para balancear gênero na classe minoritária\n",
    "minority_males = train_minority[train_minority['DEMO_RIAGENDR'] == 1]\n",
    "minority_females = train_minority[train_minority['DEMO_RIAGENDR'] == 2]\n",
    "\n",
    "\n",
    "\n",
    "if len(minority_males) < len(minority_females):\n",
    "    smote_gender = SMOTE(random_state=42)\n",
    "    X_minority = train_minority.drop('target', axis=1)\n",
    "    y_minority_gender = train_minority['DEMO_RIAGENDR']\n",
    "    \n",
    "    X_minority_balanced, _ = smote_gender.fit_resample(X_minority, y_minority_gender)\n",
    "    \n",
    "    # Adicionar target de volta\n",
    "    X_minority_balanced['target'] = 1\n",
    "    \n",
    "    # Manter apenas as novas amostras necessárias\n",
    "    n_samples_needed = len(minority_females) - len(minority_males)\n",
    "    new_samples = X_minority_balanced.tail(n_samples_needed)\n",
    "    \n",
    "    # Combinar com dados originais\n",
    "    train_data_balanced = pd.concat([train_majority, train_minority, new_samples])\n",
    "\n",
    "print(\"\\nDistribuição de gênero na classe minoritária após balanceamento:\")\n",
    "gender_dist = pd.crosstab(train_data_balanced[train_data_balanced['target']==1]['DEMO_RIAGENDR'], columns='count')\n",
    "print(gender_dist)\n",
    "\n",
    "# Preparar dados para o pipeline final\n",
    "X_train_final = train_data_balanced.drop('target', axis=1)\n",
    "y_train_final = train_data_balanced['target']\n",
    "\n",
    "# Criar pipeline final\n",
    "pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        random_state=42,\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=4,\n",
    "        learning_rate=0.1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Treinar o modelo\n",
    "pipeline.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Fazer predições\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Imprimir métricas\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get feature importances\n",
    "model = pipeline.named_steps['classifier']\n",
    "\n",
    "# Calcular SHAP values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Se o modelo retorna múltiplos outputs (um para cada classe), pegar apenas o da classe positiva\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "# Calcular importância absoluta média para cada feature\n",
    "feature_importance = np.abs(shap_values).mean(0)\n",
    "\n",
    "# Criar DataFrame com as importâncias SHAP\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': feature_importance\n",
    "})\n",
    "shap_df = shap_df.sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 30 features mais importantes (SHAP):\")\n",
    "print(shap_df.head(30))\n",
    "\n",
    "# Salvar para CSV\n",
    "shap_df.to_csv('shap_importance_gb.csv', index=False)\n",
    "\n",
    "# Opcional: Criar gráfico de sumário SHAP\n",
    "shap.summary_plot(shap_values, X_test, plot_size=(10,8))\n",
    "\n",
    "# Gravar o DataFrame modificado\n",
    "df.to_csv('NHANES_panic_subset_with_target_modified.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "# Ler dados de importância e selecionar top 23 features\n",
    "importance_df = pd.read_csv('shap_importance_gb.csv')\n",
    "top_23_features = importance_df.head(30)['feature'].tolist()\n",
    "\n",
    "# Criar lista de features incluindo DEMO_RIAGENDR se não estiver nas top 23\n",
    "features_for_balancing = top_23_features.copy()\n",
    "if 'DEMO_RIAGENDR' not in features_for_balancing:\n",
    "    features_for_balancing.append('DEMO_RIAGENDR')\n",
    "\n",
    "# Ler dataset original\n",
    "df = pd.read_csv('NHANES_panic_subset_with_target_modified.csv')\n",
    "\n",
    "\n",
    "# Separar colunas numéricas e categóricas\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Pré-processar dados\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Imputar valores numéricos coluna por coluna\n",
    "for col in numeric_cols:\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_processed[col] = imputer.fit_transform(df_processed[[col]])\n",
    "\n",
    "# Processar dados categóricos\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    # Primeiro imputar valores faltantes\n",
    "    df_processed[col] = df_processed[col].fillna('missing')\n",
    "    # Depois aplicar LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Selecionar features para balanceamento e target\n",
    "X_balance = df_processed[features_for_balancing]\n",
    "y = df_processed['target']\n",
    "\n",
    "# Primeiro dividir dados em treino e teste para evitar data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balance, y, test_size=0.35, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Criar DataFrame de treino completo\n",
    "train_data = X_train.copy()\n",
    "train_data['target'] = y_train\n",
    "\n",
    "# Separar classe minoritária\n",
    "train_minority = train_data[train_data['target'] == 1]\n",
    "train_majority = train_data[train_data['target'] == 0]\n",
    "\n",
    "print(\"\\nDistribuição de gênero na classe minoritária antes do balanceamento:\")\n",
    "print(pd.crosstab(train_minority['DEMO_RIAGENDR'], columns='count'))\n",
    "\n",
    "# SMOTE para balancear gênero na classe minoritária\n",
    "minority_males = train_minority[train_minority['DEMO_RIAGENDR'] == 1]\n",
    "minority_females = train_minority[train_minority['DEMO_RIAGENDR'] == 2]\n",
    "\n",
    "if len(minority_males) < len(minority_females):\n",
    "    smote_gender = SMOTE(random_state=42)\n",
    "    X_minority = train_minority.drop('target', axis=1)\n",
    "    y_minority_gender = train_minority['DEMO_RIAGENDR']\n",
    "    \n",
    "    X_minority_balanced, _ = smote_gender.fit_resample(X_minority, y_minority_gender)\n",
    "    \n",
    "    # Adicionar target de volta\n",
    "    X_minority_balanced['target'] = 1\n",
    "    \n",
    "    # Manter apenas as novas amostras necessárias\n",
    "    n_samples_needed = len(minority_females) - len(minority_males)\n",
    "    new_samples = X_minority_balanced.tail(n_samples_needed)\n",
    "    \n",
    "    # Combinar com dados originais\n",
    "    train_data_balanced = pd.concat([train_majority, train_minority, new_samples])\n",
    "\n",
    "print(\"\\nDistribuição de gênero na classe minoritária após balanceamento:\")\n",
    "gender_dist = pd.crosstab(train_data_balanced[train_data_balanced['target']==1]['DEMO_RIAGENDR'], columns='count')\n",
    "print(gender_dist)\n",
    "\n",
    "# Preparar dados para o pipeline final \n",
    "\n",
    "#['SXQ_SXQ120', 'RHQ_RHQ370', 'MCQ_MCQ180B' HUQ_HUQ060\n",
    "\n",
    "X_train_final = train_data_balanced.drop(['target',\"DEMO_RIAGENDR\",\n",
    "    \"OCQ_OCD390\",\n",
    "    \"SXQ_SXQ020\",\n",
    "    \"MCQ_MCQ250C\",\n",
    "    \"AUQ_AUQ130\",\n",
    "    \"BPQ_BPQ040D\",\n",
    "    \"MCQ_MCQ230A\",\n",
    "    \"PH_PHAFSTMN\", \"SSMUMP_WTSSMC2Y\", \"BMX_BMXSUB\",\n",
    "    \"AUQ_AUQ220\"] if 'DEMO_RIAGENDR' not in top_23_features else ['target'], axis=1)\n",
    "y_train_final = train_data_balanced['target']\n",
    "\n",
    "# Ajustar X_test também\n",
    "\n",
    "X_test_final = X_test.drop(['DEMO_RIAGENDR',\n",
    "    \"OCQ_OCD390\",\n",
    "    \"SXQ_SXQ020\",\n",
    "    \"MCQ_MCQ250C\",\n",
    "    \"AUQ_AUQ130\",\n",
    "    \"BPQ_BPQ040D\",\n",
    "    \"MCQ_MCQ230A\",\n",
    "    \"PH_PHAFSTMN\", \"SSMUMP_WTSSMC2Y\", \"BMX_BMXSUB\",\n",
    "    \"AUQ_AUQ220\"], axis=1) if 'DEMO_RIAGENDR' not in top_23_features else X_test\n",
    "\n",
    "# Criar pipeline final\n",
    "pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        random_state=42,\n",
    "        min_samples_leaf=1,\n",
    "        min_samples_split=4,\n",
    "        learning_rate=0.1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Treinar o modelo\n",
    "pipeline.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Fazer predições\n",
    "y_pred = pipeline.predict(X_test_final)\n",
    "\n",
    "# Imprimir métricas\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get feature importances\n",
    "model = pipeline.named_steps['classifier']\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df_new = pd.DataFrame({\n",
    "    'feature': X_train_final.columns,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "importance_df_new = importance_df_new.sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop features importances:\")\n",
    "print(importance_df_new)\n",
    "\n",
    "# Save to CSV\n",
    "importance_df_new.to_csv('feature_importance_gb_new.csv', index=False)\n",
    "\n",
    "# Create visualization of new feature importances\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.barh(range(len(importance_df_new)), importance_df_new['importance']*100)\n",
    "plt.yticks(range(len(importance_df_new)), importance_df_new['feature'])\n",
    "plt.xlabel('Importance (%)')\n",
    "plt.title('Feature Importance Distribution (33 Features)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_33features.png')\n",
    "plt.show()\n",
    "\n",
    "# Criar Confusion Matrix\n",
    "plt.figure(figsize=(10,7))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['no panic', 'With panic'])\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_33features.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carregar o dataset\n",
    "df = pd.read_csv('NHANES_panic_subset_with_target_modified.csv')\n",
    "\n",
    "# Mapeamentos para variáveis categóricas\n",
    "category_maps = {\n",
    "    'DEMO_RIAGENDR': {1: 'Male', 2: 'Female'},\n",
    "    'DEMO_DMDEDUC': {\n",
    "        1: 'Less Than High School', \n",
    "        2: 'High School Diploma', \n",
    "        3: 'More Than High School'\n",
    "    },\n",
    "    'DEMO_INDHHINC': {\n",
    "        1: '$0 to $4,999',  # Combina 1 e 2\n",
    "        3: '$5,000 to $9,999',  # Combina 3 e 4\n",
    "        5: '$10,000 to $14,999',  # Combina 5 e 6\n",
    "        7: '$15,000 to $19,999',  # Combina 7 e 8\n",
    "        11: '$20,000 to $34,999',  # Usa categoria existente\n",
    "        12: '$35,000 to $54,999',  # Combina algumas categorias\n",
    "        16: '$55,000 and Over'  # Mantém categoria existente\n",
    "    }\n",
    "}\n",
    "\n",
    "# Função para calcular estatísticas com percentagens dentro do grupo\n",
    "def calculate_group_stats(df, var, is_categorical=False):\n",
    "    # Converter coluna para tipo categórico, tratando NaNs\n",
    "    if is_categorical:\n",
    "        # Filtrar categorias indesejadas para variáveis específicas\n",
    "        if var == 'DEMO_DMDEDUC':\n",
    "            df = df[df[var].isin([1, 2, 3])]\n",
    "        elif var == 'DEMO_INDHHINC':\n",
    "            # Mapear valores para novos intervalos\n",
    "            income_mapping = {\n",
    "                1: '$0 to $4,999',\n",
    "                2: '$0 to $4,999',\n",
    "                3: '$5,000 to $9,999',\n",
    "                4: '$5,000 to $9,999',\n",
    "                5: '$10,000 to $14,999',\n",
    "                6: '$10,000 to $14,999',\n",
    "                7: '$15,000 to $19,999',\n",
    "                8: '$15,000 to $19,999',\n",
    "                9: '$20,000 to $34,999',\n",
    "                10: '$20,000 to $34,999',\n",
    "                11: '$20,000 to $34,999',\n",
    "                12: '$35,000 to $54,999',\n",
    "                13: '$35,000 to $54,999',\n",
    "                14: '$35,000 to $54,999',\n",
    "                15: '$55,000 and Over',\n",
    "                16: '$55,000 and Over'\n",
    "            }\n",
    "            df['DEMO_INDHHINC_grouped'] = df['DEMO_INDHHINC'].map(income_mapping)\n",
    "            var = 'DEMO_INDHHINC_grouped'\n",
    "        \n",
    "        # Converter para categoria, tratando NaNs\n",
    "        df[var] = df[var].fillna('Unknown').astype('category')\n",
    "        \n",
    "        # Contagem total e percentagem global\n",
    "        total_counts = df[var].value_counts()\n",
    "        total_percents = total_counts / len(df) * 100\n",
    "        \n",
    "        # Estatísticas para grupo normal\n",
    "        normal_counts = df[df['target'] == 0][var].value_counts()\n",
    "        normal_percents = normal_counts / len(df[df['target'] == 0]) * 100\n",
    "        \n",
    "        # Estatísticas para grupo com pânico\n",
    "        panic_counts = df[df['target'] == 1][var].value_counts()\n",
    "        panic_percents = panic_counts / len(df[df['target'] == 1]) * 100\n",
    "        \n",
    "        # Combinar resultados\n",
    "        results = []\n",
    "        for category in df[var].cat.categories:\n",
    "            # Mapear código numérico para label descritiva\n",
    "            category_label = category_maps.get(var, {}).get(category, str(category))\n",
    "            results.append([\n",
    "                category_label,\n",
    "                f\"{total_counts.get(category, 0)} ({total_percents.get(category, 0):.1f}%)\",\n",
    "                f\"{normal_counts.get(category, 0)} ({normal_percents.get(category, 0):.1f}%)\",\n",
    "                f\"{panic_counts.get(category, 0)} ({panic_percents.get(category, 0):.1f}%)\"\n",
    "            ])\n",
    "        return results\n",
    "    else:\n",
    "        # Para variáveis contínuas\n",
    "        total_mean = df[var].mean()\n",
    "        total_std = df[var].std()\n",
    "        \n",
    "        normal_mean = df[df['target'] == 0][var].mean()\n",
    "        normal_std = df[df['target'] == 0][var].std()\n",
    "        \n",
    "        panic_mean = df[df['target'] == 1][var].mean()\n",
    "        panic_std = df[df['target'] == 1][var].std()\n",
    "        \n",
    "        return [\n",
    "            f\"{total_mean:.1f} ± {total_std:.1f}\",\n",
    "            f\"{normal_mean:.1f} ± {normal_std:.1f}\",\n",
    "            f\"{panic_mean:.1f} ± {panic_std:.1f}\"\n",
    "        ]\n",
    "\n",
    "# Variáveis a analisar\n",
    "continuous_vars = {\n",
    "    'DEMO_RIDAGEEX': 'Age (years)',\n",
    "    'BMX_BMXWT': 'Weight (kg)',\n",
    "    'BMX_BMXHT': 'Height (cm)',\n",
    "    'BMX_BMXBMI': 'BMI'\n",
    "}\n",
    "\n",
    "categorical_vars = {\n",
    "    'DEMO_RIAGENDR': 'Gender',\n",
    "    'DEMO_DMDEDUC': 'Education Level',\n",
    "    'DEMO_INDHHINC': 'Household Income'\n",
    "}\n",
    "\n",
    "# Gerar resultados\n",
    "print(\"Population Characteristics:\")\n",
    "\n",
    "# Variáveis contínuas\n",
    "for var, label in continuous_vars.items():\n",
    "    if var == 'DEMO_RIDAGEEX':\n",
    "        # Converter de meses para anos\n",
    "        df[var] = df[var] / 12\n",
    "    \n",
    "    stats = calculate_group_stats(df, var)\n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"Total: {stats[0]}\")\n",
    "    print(f\"Normal: {stats[1]}\")\n",
    "    print(f\"Panic Group: {stats[2]}\")\n",
    "\n",
    "# Variáveis categóricas\n",
    "for var, label in categorical_vars.items():\n",
    "    print(f\"\\n{label}:\")\n",
    "    results = calculate_group_stats(df, var, is_categorical=True)\n",
    "    for row in results:\n",
    "        print(row)\n",
    "\n",
    "# Estatísticas adicionais\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "print(f\"Total Sample Size: {len(df)}\")\n",
    "print(f\"Normal Group Size: {len(df[df['target'] == 0])}\")\n",
    "print(f\"Panic Group Size: {len(df[df['target'] == 1])}\")\n",
    "print(f\"Panic Group Prevalence: {len(df[df['target'] == 1])/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Carregar o dataset\n",
    "df = pd.read_csv('NHANES_panic_subset_with_target_modified.csv')\n",
    "\n",
    "# Função para calcular estatísticas com percentagens dentro do grupo\n",
    "def calculate_group_stats(df, var, is_categorical=False):\n",
    "    # Estatísticas para todo o dataset\n",
    "    total_stats = []\n",
    "    \n",
    "    if is_categorical:\n",
    "        # Contagem total e percentagem global\n",
    "        total_counts = df[var].value_counts()\n",
    "        total_percents = total_counts / len(df) * 100\n",
    "        \n",
    "        # Estatísticas para grupo normal\n",
    "        normal_counts = df[df['target'] == 0][var].value_counts()\n",
    "        normal_percents = normal_counts / len(df[df['target'] == 0]) * 100\n",
    "        \n",
    "        # Estatísticas para grupo com pânico\n",
    "        panic_counts = df[df['target'] == 1][var].value_counts()\n",
    "        panic_percents = panic_counts / len(df[df['target'] == 1]) * 100\n",
    "        \n",
    "        # Combinar resultados\n",
    "        results = []\n",
    "        for category in sorted(df[var].unique()):\n",
    "            results.append([\n",
    "                f\"{category}\",\n",
    "                f\"{total_counts.get(category, 0)} ({total_percents.get(category, 0):.1f}%)\",\n",
    "                f\"{normal_counts.get(category, 0)} ({normal_percents.get(category, 0):.1f}%)\",\n",
    "                f\"{panic_counts.get(category, 0)} ({panic_percents.get(category, 0):.1f}%)\"\n",
    "            ])\n",
    "        return results\n",
    "    else:\n",
    "        # Para variáveis contínuas\n",
    "        total_mean = df[var].mean()\n",
    "        total_std = df[var].std()\n",
    "        \n",
    "        normal_mean = df[df['target'] == 0][var].mean()\n",
    "        normal_std = df[df['target'] == 0][var].std()\n",
    "        \n",
    "        panic_mean = df[df['target'] == 1][var].mean()\n",
    "        panic_std = df[df['target'] == 1][var].std()\n",
    "        \n",
    "        return [\n",
    "            f\"{total_mean:.1f} ± {total_std:.1f}\",\n",
    "            f\"{normal_mean:.1f} ± {normal_std:.1f}\",\n",
    "            f\"{panic_mean:.1f} ± {panic_std:.1f}\"\n",
    "        ]\n",
    "\n",
    "# Variáveis a analisar\n",
    "categorical_vars = {\n",
    "    'DEMO_RIAGENDR': 'Gender',\n",
    "    'DEMO_DMDEDUC': 'Education Level',\n",
    "    'DEMO_INDHHINC': 'Household Income'\n",
    "}\n",
    "\n",
    "continuous_vars = {\n",
    "    'DEMO_RIDAGEEX': 'Age (years)',\n",
    "    'BMX_BMXWT': 'Weight (kg)',\n",
    "    'BMX_BMXHT': 'Height (cm)',\n",
    "    'BMX_BMXBMI': 'BMI'\n",
    "}\n",
    "\n",
    "# Gerar resultados\n",
    "print(\"Population Characteristics:\")\n",
    "\n",
    "# Variáveis contínuas\n",
    "for var, label in continuous_vars.items():\n",
    "    divisor = 12 if var == 'DEMO_RIDAGEEX' else 1\n",
    "    stats = calculate_group_stats(df, var)\n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"Total: {stats[0]}\")\n",
    "    print(f\"Normal: {stats[1]}\")\n",
    "    print(f\"Panic Group: {stats[2]}\")\n",
    "\n",
    "# Variáveis categóricas\n",
    "for var, label in categorical_vars.items():\n",
    "    print(f\"\\n{label}:\")\n",
    "    results = calculate_group_stats(df, var, is_categorical=True)\n",
    "    for row in results:\n",
    "        print(row)\n",
    "\n",
    "# Estatísticas adicionais\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "print(f\"Total Sample Size: {len(df)}\")\n",
    "print(f\"Normal Group Size: {len(df[df['target'] == 0])}\")\n",
    "print(f\"Panic Group Size: {len(df[df['target'] == 1])}\")\n",
    "print(f\"Panic Group Prevalence: {len(df[df['target'] == 1])/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('NHANES_panic_subset_with_target_modified.csv')\n",
    "\n",
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregar o dataset\n",
    "df = pd.read_csv('NHANES_panic_subset_with_target_modified.csv')\n",
    "\n",
    "# Converter idade de meses para anos\n",
    "df['Age'] = df['DEMO_RIDAGEEX'] / 12\n",
    "\n",
    "# Configurar o estilo do gráfico\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Criar boxplot\n",
    "sns.boxplot(x='target', y='Age', data=df, \n",
    "          palette=['skyblue', 'lightcoral'],\n",
    "          medianprops={\"color\": \"darkred\", \"linewidth\": 2},\n",
    "          boxprops={\"edgecolor\": \"navy\", \"alpha\": 0.7},\n",
    "          whiskerprops={\"color\": \"navy\", \"linestyle\": \"--\"})\n",
    "\n",
    "# Personalizar o gráfico\n",
    "plt.title('Age Distribution: Normal vs Panic Group', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Group', fontsize=12)\n",
    "plt.ylabel('Age (years)', fontsize=12)\n",
    "plt.xticks([0, 1], ['Normal', 'Panic Group'])\n",
    "\n",
    "# Adicionar descrição estatística (atualize os valores conforme necessário)\n",
    "plt.text(0.02, 0.95, 'Normal Group\\nMean: 29.5\\nStd: 5.8', \n",
    "       transform=plt.gca().transAxes, \n",
    "       verticalalignment='top',\n",
    "       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "plt.text(0.7, 0.95, 'Panic Group\\nMean: 31.1\\nStd: 4.7', \n",
    "       transform=plt.gca().transAxes, \n",
    "       verticalalignment='top',\n",
    "       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Após criar o boxplot, adicione:\n",
    "# Criar mini-tabela de estatísticas\n",
    "stats_table = pd.DataFrame({\n",
    "   'Group': ['Normal', 'Panic Group'],\n",
    "   'Mean Age': [\n",
    "       df[df['target'] == 0]['Age'].mean(),\n",
    "       df[df['target'] == 1]['Age'].mean()\n",
    "   ],\n",
    "   'Std Dev': [\n",
    "       df[df['target'] == 0]['Age'].std(),\n",
    "       df[df['target'] == 1]['Age'].std()\n",
    "   ],\n",
    "   '25th Percentile': [\n",
    "       df[df['target'] == 0]['Age'].quantile(0.25),\n",
    "       df[df['target'] == 1]['Age'].quantile(0.25)\n",
    "   ],\n",
    "   '75th Percentile': [\n",
    "       df[df['target'] == 0]['Age'].quantile(0.75),\n",
    "       df[df['target'] == 1]['Age'].quantile(0.75)\n",
    "   ]\n",
    "})\n",
    "\n",
    "# Formatar valores numéricos para 2 casas decimais\n",
    "stats_table = stats_table.round(2)\n",
    "\n",
    "# Exibir mini-tabela\n",
    "print(\"\\nAge Distribution Statistics:\")\n",
    "print(stats_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregar o dataset\n",
    "df = pd.read_csv('NHANES_panic_subset_with_target_modified.csv')\n",
    "\n",
    "# Mapeamentos para labels\n",
    "gender_map = {1: 'Male', 2: 'Female'}\n",
    "education_map = {\n",
    "   1: 'Less Than High School', \n",
    "   2: 'High School Diploma', \n",
    "   3: 'More Than High School'\n",
    "}\n",
    "\n",
    "# Preparar dados para gráfico\n",
    "def prepare_categorical_data(column, mapping):\n",
    "   df[f'{column}_mapped'] = df[column].map(mapping)\n",
    "   proportions = df.groupby(['target', f'{column}_mapped']).size().unstack(fill_value=0)\n",
    "   proportions_percent = proportions.apply(lambda x: x / x.sum() * 100, axis=1)\n",
    "   return proportions_percent\n",
    "\n",
    "# Configurações de estilo\n",
    "plt.figure(figsize=(16, 7), dpi=300)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Gráfico de Gênero\n",
    "plt.subplot(1, 2, 1)\n",
    "gender_props = prepare_categorical_data('DEMO_RIAGENDR', gender_map)\n",
    "gender_props.plot(kind='bar', ax=plt.gca(), width=0.8)\n",
    "plt.title('A. Gender Distribution', fontsize=20, fontweight='bold')\n",
    "plt.xlabel('Group', fontsize=18)\n",
    "plt.ylabel('Percentage', fontsize=18)\n",
    "plt.legend(title='Gender', fontsize=16, title_fontsize=16)\n",
    "plt.xticks([0, 1], ['Normal', 'Panic'] ,fontsize=14, rotation=0)\n",
    "\n",
    "# Gráfico de Educação\n",
    "plt.subplot(1, 2, 2)\n",
    "education_props = prepare_categorical_data('DEMO_DMDEDUC', education_map)\n",
    "education_props = education_props[['Less Than High School', 'High School Diploma', 'More Than High School']]\n",
    "education_props.plot(kind='bar', ax=plt.gca(), width=0.8)\n",
    "plt.title('B. Education Level Distribution', fontsize=20, fontweight='bold')\n",
    "plt.xlabel('Group', fontsize=18)\n",
    "plt.ylabel('Percentage', fontsize=18)\n",
    "plt.legend(title='Education', fontsize=16, title_fontsize=16)\n",
    "plt.xticks([0, 1], ['Normal', 'Panic'],fontsize=14, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "\n",
    "def evaluate_and_plot_model(pipeline, X_train, y_train, X_test, y_test, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluate the model using cross-validation on training data and final evaluation on test data.\n",
    "    \"\"\"\n",
    "    # Set up cross-validation\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Perform cross-validation on training data\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Final evaluation on test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'cv_scores': cv_scores,\n",
    "        'cv_mean_score': cv_scores.mean(),\n",
    "        'cv_std_score': cv_scores.std(),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'probabilities': y_pred_proba,\n",
    "        'y_true': y_test,\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    # Plotting\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'axes.labelsize': 14,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.fontsize': 14\n",
    "    })\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = fig.add_gridspec(3, 2)\n",
    "    \n",
    "    # 1. Cross-validation Results\n",
    "    ax_cv = fig.add_subplot(gs[0, :])\n",
    "    cv_data = pd.DataFrame({'Fold': [f'Fold {i+1}' for i in range(n_splits)], 'ROC AUC': results['cv_scores']})\n",
    "    sns.barplot(data=cv_data, x='Fold', y='ROC AUC', ax=ax_cv, color='skyblue')\n",
    "    ax_cv.axhline(y=results['cv_mean_score'], color='red', linestyle='--', \n",
    "                  label=f'Mean CV Score: {results[\"cv_mean_score\"]:.3f}')\n",
    "    ax_cv.fill_between([-0.5, n_splits-0.5], \n",
    "                      results['cv_mean_score'] - 2*results['cv_std_score'],\n",
    "                      results['cv_mean_score'] + 2*results['cv_std_score'],\n",
    "                      alpha=0.2, color='red',\n",
    "                      label=f'±2 STD: {2*results[\"cv_std_score\"]:.3f}')\n",
    "    ax_cv.set_title('Cross-validation Results by Fold', fontsize=18, pad=20)\n",
    "    ax_cv.set_ylim([min(results['cv_scores'])-0.05, max(results['cv_scores'])+0.05])\n",
    "    ax_cv.legend(fontsize=14)\n",
    "    \n",
    "    # 2. Class Accuracy\n",
    "    ax_acc = fig.add_subplot(gs[1, 0])\n",
    "    tn, fp, fn, tp = results['confusion_matrix'].ravel()\n",
    "    accuracy_0 = tn / (tn + fp)\n",
    "    accuracy_1 = tp / (tp + fn)\n",
    "    performance_data = pd.DataFrame({\n",
    "        'Class': ['Negative (0)', 'Positive (1)'],\n",
    "        'Accuracy': [accuracy_0, accuracy_1]\n",
    "    })\n",
    "    sns.barplot(data=performance_data, x='Class', y='Accuracy', ax=ax_acc)\n",
    "    ax_acc.set_title('Accuracy by Class', fontsize=18)\n",
    "    ax_acc.set_ylim([0, 1])\n",
    "    for i, v in enumerate(performance_data['Accuracy']):\n",
    "        ax_acc.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Confusion Matrix\n",
    "    ax_cm = fig.add_subplot(gs[1, 1])\n",
    "    sns.heatmap(results['confusion_matrix'], annot=True, fmt='d', cmap='Blues', \n",
    "                ax=ax_cm, annot_kws={'size': 14})\n",
    "    ax_cm.set_title('Confusion Matrix', fontsize=18)\n",
    "    ax_cm.set_xlabel('Predicted', fontsize=14)\n",
    "    ax_cm.set_ylabel('Actual', fontsize=14)\n",
    "    \n",
    "    # 4. Probability Distribution\n",
    "    ax_prob = fig.add_subplot(gs[2, 0])\n",
    "    sns.histplot(data=pd.DataFrame({\n",
    "        'Probability': results['probabilities'],\n",
    "        'True Class': results['y_true']\n",
    "    }), x='Probability', hue='True Class', bins=30, ax=ax_prob)\n",
    "    ax_prob.set_title('Prediction Probability Distribution', fontsize=18)\n",
    "    \n",
    "    # 5. ROC Curve\n",
    "    ax_roc = fig.add_subplot(gs[2, 1])\n",
    "    fpr, tpr, _ = roc_curve(results['y_true'], results['probabilities'])\n",
    "    ax_roc.plot(fpr, tpr, linewidth=2)\n",
    "    ax_roc.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "    ax_roc.set_title(f'ROC Curve (AUC = {results[\"roc_auc\"]:.3f})', fontsize=18)\n",
    "    ax_roc.set_xlabel('False Positive Rate', fontsize=14)\n",
    "    ax_roc.set_ylabel('True Positive Rate', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uso da função\n",
    "results_panic = evaluate_and_plot_model(pipeline, X_train_final, y_train_final, X_test_final, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair informações relevantes para a figure caption\n",
    "cv_mean = results_panic['cv_mean_score']\n",
    "cv_std = results_panic['cv_std_score']\n",
    "test_auc = results_panic['roc_auc']\n",
    "tn, fp, fn, tp = results_panic['confusion_matrix'].ravel()\n",
    "accuracy_0 = tn / (tn + fp)\n",
    "accuracy_1 = tp / (tp + fn)\n",
    "overall_accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "\n",
    "# Informações sobre a distribuição de probabilidades\n",
    "probabilities = results_panic['probabilities']\n",
    "mean_prob = np.mean(probabilities)\n",
    "median_prob = np.median(probabilities)\n",
    "\n",
    "# Imprimir as informações\n",
    "print(f\"Cross-validation mean ROC AUC: {cv_mean:.3f} ± {cv_std:.3f}\")\n",
    "print(f\"Test set ROC AUC: {test_auc:.3f}\")\n",
    "print(f\"Accuracy for class 0 (Negative): {accuracy_0:.3f}\")\n",
    "print(f\"Accuracy for class 1 (Positive): {accuracy_1:.3f}\")\n",
    "print(f\"Overall accuracy: {overall_accuracy:.3f}\")\n",
    "print(f\"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
    "print(f\"Mean probability: {mean_prob:.3f}\")\n",
    "print(f\"Median probability: {median_prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Certificar-se de usar apenas features numéricas\n",
    "X_explain = X_test_final.select_dtypes(include=[np.number])\n",
    "\n",
    "# Criar explicador SHAP\n",
    "model = pipeline.named_steps['classifier']\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calcular SHAP values\n",
    "shap_values = explainer.shap_values(X_explain)\n",
    "\n",
    "# Garantir que estamos usando os valores da classe positiva para binário\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "# 1. Salvar valores de SHAP\n",
    "pd.DataFrame(\n",
    "    shap_values, \n",
    "    columns=X_explain.columns\n",
    ").to_csv('shap_values.csv', index=False)\n",
    "\n",
    "# 2. Gerar summary plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "shap.summary_plot(shap_values, X_explain, \n",
    "                  feature_names=X_explain.columns.tolist(), \n",
    "                  show=False)\n",
    "plt.title('SHAP Summary Plot for Panic Disorder Prediction')\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_summary_plot.png')\n",
    "plt.close()\n",
    "\n",
    "# 3. Gerar bar plot de importância\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_explain, \n",
    "                  plot_type=\"bar\", \n",
    "                  feature_names=X_explain.columns.tolist(), \n",
    "                  show=False)\n",
    "plt.title('SHAP Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# 4. Calcular importância média das features\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "shap_importance_df = pd.DataFrame({\n",
    "    'feature': X_explain.columns,\n",
    "    'mean_shap_importance': shap_importance\n",
    "}).sort_values('mean_shap_importance', ascending=False)\n",
    "\n",
    "shap_importance_df.to_csv('shap_feature_importance.csv', index=False)\n",
    "\n",
    "print(\"Arquivos gerados:\")\n",
    "print(\"1. shap_values.csv\")\n",
    "print(\"2. shap_summary_plot.png\")\n",
    "print(\"3. shap_feature_importance.png\")\n",
    "print(\"4. shap_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Configurações de estilo\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Criar figura com subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 20))\n",
    "fig.suptitle('SHAP Analysis for Panic Disorder Prediction', fontsize=16, fontweight='bold')\n",
    "\n",
    "data = {\n",
    "    \"Variable\": [\n",
    "        \"CVX_CVDEXSTS\",\n",
    "        \"WHQ_WHD120\",\n",
    "        \"BIX_BIDFAT\",\n",
    "        \"SMQ_SMD030\",\n",
    "        \"BMX_BMXTRI\",\n",
    "        \"DEMO_INDFMINC\",\n",
    "        \"BPX_PEASCTM1\",\n",
    "        \"DEMO_INDFMPIR\",\n",
    "        \"IMQ_IMQ020\",\n",
    "        \"BPX_BPXDAR\",\n",
    "        \"WHQ_WHD050\",\n",
    "        \"DEMO_RIDAGEMN\",\n",
    "        \"MPQ_MPQ070\",\n",
    "        \"BPQ_BPQ010\",\n",
    "        \"ALQ_ALQ150\",\n",
    "        \"MCQ_MCQ250F\"\n",
    "    ],\n",
    "    \"Category\": [\n",
    "        \"Cardiovascular Fitness\",\n",
    "        \"Weight History\",\n",
    "        \"Body Composition\",\n",
    "        \"Smoking - Cigarette/Tobacco Use\",\n",
    "        \"Body Measures\",\n",
    "        \"Demographics - Socioeconomic\",\n",
    "        \"Blood Pressure\",\n",
    "        \"Demographics - Socioeconomic\",\n",
    "        \"Immunization\",\n",
    "        \"Blood Pressure\",\n",
    "        \"Weight History\",\n",
    "        \"Demographics - Age\",\n",
    "        \"Pain\",\n",
    "        \"Blood Pressure & Cholesterol\",\n",
    "        \"Alcohol Use\",\n",
    "        \"Medical Conditions - Family History\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# SHAP Analysis\n",
    "# Garantir que todos os dados sejam numéricos\n",
    "X_explain = X_test_final.select_dtypes(include=[np.number])\n",
    "\n",
    "# Criar explicador SHAP\n",
    "model = pipeline.named_steps['classifier']\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calcular SHAP values\n",
    "shap_values = explainer.shap_values(X_explain)\n",
    "if isinstance(shap_values, list):\n",
    "   shap_values = shap_values[1]  # Selecionar classe positiva\n",
    "\n",
    "# Calcular importância média de SHAP\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "shap_df = pd.DataFrame({\n",
    "   'Variable': X_explain.columns,\n",
    "   'SHAP_Importance': shap_importance\n",
    "})\n",
    "\n",
    "# Adicionar categoria ao DataFrame SHAP\n",
    "shap_df['Category'] = shap_df['Variable'].map(dict(zip(data['Variable'], data['Category'])))\n",
    "\n",
    "# Calcular média de SHAP por categoria\n",
    "category_shap_importance = shap_df.groupby('Category')['SHAP_Importance'].max()\n",
    "\n",
    "# Ordenar categorias por sua máxima importância SHAP\n",
    "sorted_categories = category_shap_importance.sort_values(ascending=False).index\n",
    "\n",
    "# Preparar DataFrame para plotagem\n",
    "shap_df_sorted = []\n",
    "for category in sorted_categories:\n",
    "   category_df = shap_df[shap_df['Category'] == category].sort_values('SHAP_Importance', ascending=False)\n",
    "   shap_df_sorted.append(category_df)\n",
    "\n",
    "# Concatenar dataframes\n",
    "shap_df_final = pd.concat(shap_df_sorted)\n",
    "\n",
    "# 1. SHAP Feature Importance por Categoria\n",
    "sns.barplot(\n",
    "   data=shap_df_final,\n",
    "   x=\"SHAP_Importance\",\n",
    "   y=\"Variable\",\n",
    "   hue=\"Category\",\n",
    "   dodge=False,\n",
    "   palette=\"tab10\",\n",
    "   ax=ax1\n",
    ")\n",
    "ax1.set_title('SHAP Feature Importance by Category', fontsize=14)\n",
    "ax1.set_xlabel('Mean |SHAP Value|', fontsize=12)\n",
    "ax1.set_ylabel('Variables', fontsize=12)\n",
    "handles, _ = ax1.get_legend_handles_labels()\n",
    "ax1.legend(handles, shap_df_final['Category'].unique(), title=\"Category\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 2. SHAP Summary Plot \n",
    "plt.sca(ax2)\n",
    "shap.summary_plot(shap_values, X_explain, \n",
    "                 feature_names=X_explain.columns.tolist(),\n",
    "                 plot_type='dot',\n",
    "                 show=False)\n",
    "ax2.set_title('SHAP Summary Plot', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro gráfico: SHAP Feature Importance por Categoria\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "   data=shap_df_final,\n",
    "   x=\"SHAP_Importance\",\n",
    "   y=\"Variable\",\n",
    "   hue=\"Category\",\n",
    "   dodge=False,\n",
    "   palette=\"tab10\"\n",
    ")\n",
    "plt.title('SHAP Feature Importance by Category', fontsize=14, pad=20)\n",
    "plt.xlabel('Mean |SHAP Value|', fontsize=12)\n",
    "plt.ylabel('Variables', fontsize=12)\n",
    "handles, _ = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles, shap_df_final['Category'].unique(), \n",
    "         title=\"Category\", \n",
    "         bbox_to_anchor=(1.05, 1), \n",
    "         loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Segundo gráfico: SHAP Summary Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(\n",
    "   shap_values, \n",
    "   X_explain,\n",
    "   feature_names=X_explain.columns.tolist(),\n",
    "   plot_type='dot',\n",
    "   show=True\n",
    ")\n",
    "plt.title('SHAP Summary Plot', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shap_analysis_report(shap_df_final, shap_values, X_explain):\n",
    "    report = \"# SHAP Analysis Report for Panic Disorder Prediction\\n\\n\"\n",
    "    \n",
    "    # Overview\n",
    "    report += \"## Overview\\n\"\n",
    "    report += \"This analysis explores the feature importance and impact in predicting panic disorder using SHAP (SHapley Additive exPlanations) values, derived from a Gradient Boosting Classifier trained on NHANES 1999-2004 data.\\n\\n\"\n",
    "    \n",
    "    # Feature Importance by Category\n",
    "    report += \"### SHAP Feature Importance by Category\\n\\n\"\n",
    "    report += \"#### Top Categories Influencing Panic Disorder Risk:\\n\"\n",
    "    \n",
    "    # Calculate category importance\n",
    "    category_importance = shap_df_final.groupby('Category')['SHAP_Importance'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    for i, (category, importance) in enumerate(category_importance.items(), 1):\n",
    "        report += f\"{i}. **{category}**\\n\"\n",
    "        \n",
    "        # Get top features for this category\n",
    "        category_features = shap_df_final[shap_df_final['Category'] == category].sort_values('SHAP_Importance', ascending=False)\n",
    "        top_features = category_features.head(3)\n",
    "        \n",
    "        report += f\"   - Top features: {', '.join(top_features['Variable'])}\\n\"\n",
    "        report += f\"   - Mean SHAP Importance: {importance:.4f}\\n\\n\"\n",
    "    \n",
    "    # Detailed Insights\n",
    "    report += \"#### Detailed Insights:\\n\"\n",
    "    report += \"- Top individual features demonstrate complex, multifaceted risk factors\\n\"\n",
    "    report += \"- Medical history appears more predictive than single lifestyle metrics\\n\"\n",
    "    report += \"- Physiological and behavioral factors interact in panic disorder prediction\\n\\n\"\n",
    "    \n",
    "    # SHAP Summary Plot Interpretation\n",
    "    report += \"### SHAP Summary Plot Interpretation\\n\\n\"\n",
    "    report += \"#### Feature Impact Characteristics:\\n\"\n",
    "    report += \"- Color indicates feature's impact direction\\n\"\n",
    "    report += \"- Red: Increased panic disorder risk\\n\"\n",
    "    report += \"- Blue: Decreased panic disorder risk\\n\"\n",
    "    report += \"- Horizontal spread shows magnitude of impact\\n\\n\"\n",
    "    \n",
    "    # Key Observations\n",
    "    report += \"#### Key Observations:\\n\"\n",
    "    report += \"- Most features show nuanced, non-linear relationships with panic disorder risk\\n\"\n",
    "    report += \"- Some features demonstrate high variability in their predictive power\\n\"\n",
    "    report += \"- No single feature definitively determines panic disorder risk\\n\\n\"\n",
    "    \n",
    "    # Additional Statistical Details\n",
    "    report += \"### Additional Statistical Details\\n\"\n",
    "    report += f\"- Total Features Analyzed: {len(X_explain.columns)}\\n\"\n",
    "    report += f\"- Top 5 Most Important Features:\\n\"\n",
    "    \n",
    "    top_5_features = shap_df_final.sort_values('SHAP_Importance', ascending=False).head(5)\n",
    "    for _, row in top_5_features.iterrows():\n",
    "        report += f\"  1. **{row['Variable']}** (Category: {row['Category']}): {row['SHAP_Importance']:.4f}\\n\"\n",
    "    \n",
    "    # Methodological Notes\n",
    "    report += \"\\n### Methodological Considerations\\n\"\n",
    "    report += \"- Analysis based on machine learning model trained on NHANES dataset\\n\"\n",
    "    report += \"- SHAP values provide model-agnostic feature importance\\n\"\n",
    "    report += \"- Captures complex interactions beyond traditional feature importance methods\\n\\n\"\n",
    "    \n",
    "    # Limitations\n",
    "    report += \"### Limitations\\n\"\n",
    "    report += \"- Observational study design\\n\"\n",
    "    report += \"- Potential confounding factors\\n\"\n",
    "    report += \"- Requires validation in diverse populations\\n\\n\"\n",
    "    \n",
    "    # Recommendations\n",
    "    report += \"### Recommendations\\n\"\n",
    "    report += \"1. Further investigate top predictive medical and physiological factors\\n\"\n",
    "    report += \"2. Consider personalized risk assessment incorporating multiple factors\\n\"\n",
    "    report += \"3. Develop targeted interventions addressing identified risk patterns\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Usar a função\n",
    "report = generate_shap_analysis_report(shap_df_final, shap_values, X_explain)\n",
    "\n",
    "# Salvar relatório em arquivo\n",
    "with open('shap_analysis_report.md', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Opcional: imprimir relatório\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_summary(shap_values, X_explain, max_display=12):\n",
    "    # Calcular médias absolutas dos valores SHAP para cada feature\n",
    "    feature_importance = np.abs(shap_values).mean(0)\n",
    "    feature_names = X_explain.columns.tolist()\n",
    "    \n",
    "    # Criar DataFrame com os valores SHAP\n",
    "    plot_data = []\n",
    "    for feature_idx in range(len(feature_names)):\n",
    "        feature_shap_values = shap_values[:, feature_idx]\n",
    "        feature_values = X_explain.iloc[:, feature_idx]\n",
    "        \n",
    "        for i in range(len(feature_shap_values)):\n",
    "            plot_data.append({\n",
    "                'Feature': feature_names[feature_idx],\n",
    "                'SHAP Value': feature_shap_values[i],\n",
    "                'Feature Value': feature_values[i],\n",
    "                'Importance': feature_importance[feature_idx]\n",
    "            })\n",
    "    \n",
    "    df_plot = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Ordenar features por importância\n",
    "    top_features = (df_plot.groupby('Feature')['Importance']\n",
    "                   .mean()\n",
    "                   .sort_values(ascending=True)\n",
    "                   .tail(max_display)\n",
    "                   .index)\n",
    "    \n",
    "    df_plot = df_plot[df_plot['Feature'].isin(top_features)]\n",
    "    \n",
    "    # Criar gráfico\n",
    "    fig = px.strip(\n",
    "        df_plot,\n",
    "        y='Feature',\n",
    "        x='SHAP Value',\n",
    "        color='Feature Value',\n",
    "        title='SHAP Summary Plot',\n",
    "        width=1000,  # Aumentar largura\n",
    "        height=600,  # Aumentar altura\n",
    "    )\n",
    "    \n",
    "    # Customizar layout\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        yaxis=dict(\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='LightGray',\n",
    "            tickfont=dict(size=12),  # Aumentar tamanho da fonte\n",
    "            showline=True,\n",
    "            linewidth=1,\n",
    "            linecolor='black'\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title='SHAP Value',\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='LightGray',\n",
    "            zeroline=True,\n",
    "            zerolinewidth=1.5,\n",
    "            zerolinecolor='black',\n",
    "            showline=True,\n",
    "            linewidth=1,\n",
    "            linecolor='black'\n",
    "        ),\n",
    "        margin=dict(l=200),  # Aumentar margem esquerda\n",
    "        showlegend=False,\n",
    "        height=30 * len(top_features) + 150  # Altura dinâmica baseada no número de features\n",
    "    )\n",
    "    \n",
    "    # Adicionar linhas de grade horizontais mais visíveis\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shap= pd.read_csv(\"/Users/filipecarvalho/Documents/data_science_projects/PanicPred/shap_feature_importance.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_shap.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
